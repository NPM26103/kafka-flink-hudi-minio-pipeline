services:
  # -------------------- Kafka (KRaft) --------------------
  broker:
    image: apache/kafka:3.7.0
    container_name: broker
    ports:
      - "29092:29092"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092,CONTROLLER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@broker:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0

  # Create topics
  kafka_init:
    image: apache/kafka:3.7.0
    depends_on:
      - broker
    entrypoint: ["/bin/sh","-lc"]
    command: >
      "
      echo 'Waiting Kafka...'; sleep 3;
      /opt/kafka/bin/kafka-topics.sh --bootstrap-server broker:9092 --create --if-not-exists --topic raw.student.csv --partitions 1 --replication-factor 1;
      /opt/kafka/bin/kafka-topics.sh --bootstrap-server broker:9092 --create --if-not-exists --topic raw.student.http --partitions 1 --replication-factor 1;
      /opt/kafka/bin/kafka-topics.sh --bootstrap-server broker:9092 --create --if-not-exists --topic output-topic --partitions 1 --replication-factor 1;
      /opt/kafka/bin/kafka-topics.sh --bootstrap-server broker:9092 --create --if-not-exists --topic output-topic-dlq --partitions 1 --replication-factor 1;
      echo 'Kafka topics ready.';
      "
    restart: "no"

  # -------------------- MinIO (S3 for Hudi/Hive) --------------------
  minio:
    image: minio/minio:latest
    container_name: minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data

  minio_init:
    image: minio/mc:latest
    container_name: minio-init
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -lc "
      sleep 2;
      mc alias set local http://minio:9000 minioadmin minioadmin;
      mc mb -p local/hudi || true;
      mc anonymous set none local/hudi || true;
      echo 'MinIO bucket ready: hudi';
      "
    restart: "no"

  # -------------------- Flink --------------------
  jobmanager:
    build:
      context: ./docker/flink
    image: final-flink:1.18.1
    container_name: flink-jobmanager
    command: jobmanager
    ports:
      - "8081:8081"
    environment:
      HADOOP_CONF_DIR: /opt/flink/conf
      # optional: also provide env creds (some libs read env first)
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: jobmanager
        jobmanager.memory.process.size: 1600m
        parallelism.default: 1
        taskmanager.numberOfTaskSlots: 2
        state.backend: hashmap
        state.checkpoints.dir: file:///data/flink-checkpoints
        state.savepoints.dir: file:///data/flink-savepoints

        fs.s3a.endpoint: http://minio:9000
        fs.s3a.access.key: minioadmin
        fs.s3a.secret.key: minioadmin
        fs.s3a.path.style.access: true
        fs.s3a.connection.ssl.enabled: false
        fs.s3a.aws.credentials.provider: org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
        fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem
        fs.AbstractFileSystem.s3a.impl: org.apache.hadoop.fs.s3a.S3A
    volumes:
      - ./docker/data:/data
    depends_on:
      - broker
      - kafka_init
      - minio
      - minio_init

  taskmanager1:
    image: final-flink:1.18.1
    command: taskmanager
    environment:
      HADOOP_CONF_DIR: /opt/flink/conf
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 2
        taskmanager.memory.process.size: 2000m

        fs.s3a.endpoint: http://minio:9000
        fs.s3a.access.key: minioadmin
        fs.s3a.secret.key: minioadmin
        fs.s3a.path.style.access: true
        fs.s3a.connection.ssl.enabled: false
        fs.s3a.aws.credentials.provider: org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
        fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem
        fs.AbstractFileSystem.s3a.impl: org.apache.hadoop.fs.s3a.S3A
    volumes:
      - ./docker/data:/data
    depends_on:
      - jobmanager

  taskmanager2:
    image: final-flink:1.18.1
    command: taskmanager
    environment:
      HADOOP_CONF_DIR: /opt/flink/conf
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 2
        taskmanager.memory.process.size: 2000m

        fs.s3a.endpoint: http://minio:9000
        fs.s3a.access.key: minioadmin
        fs.s3a.secret.key: minioadmin
        fs.s3a.path.style.access: true
        fs.s3a.connection.ssl.enabled: false
        fs.s3a.aws.credentials.provider: org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
        fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem
        fs.AbstractFileSystem.s3a.impl: org.apache.hadoop.fs.s3a.S3A
    volumes:
      - ./docker/data:/data
    depends_on:
      - jobmanager

  # -------------------- Auto-submit Flink transform job --------------------
  flink_job:
    image: final-flink:1.18.1
    container_name: flink-submit-flink
    environment:
      HADOOP_CONF_DIR: /opt/flink/conf
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
    depends_on:
      - jobmanager
      - taskmanager1
      - taskmanager2
      - broker
      - kafka_init
    command: ["/submit-flink.sh"]
    restart: "no"

  # -------------------- Auto-submit Hudi job --------------------
  hudi_job:
    image: final-flink:1.18.1
    container_name: flink-submit-hudi
    environment:
      HADOOP_CONF_DIR: /opt/flink/conf
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
    depends_on:
      - jobmanager
      - taskmanager1
      - taskmanager2
      - broker
      - kafka_init
      - minio_init
      # - hive-metastore
    command: ["/submit-hudi.sh"]
    restart: "no"

  # -------------------- Hive Metastore DB --------------------
  # metastore-db:
  #   image: postgres:15
  #   container_name: metastore-db
  #   environment:
  #     POSTGRES_DB: metastore
  #     POSTGRES_USER: hive
  #     POSTGRES_PASSWORD: hive
  #   ports:
  #     - "5433:5432"
  #   volumes:
  #     - metastore_db:/var/lib/postgresql/data
  #   healthcheck:
  #     test: ["CMD-SHELL", "pg_isready -U hive -d metastore"]
  #     interval: 3s
  #     timeout: 3s
  #     retries: 20

  # # -------------------- Init Metastore Schema (run once) --------------------
  # hive_metastore_init:
  #   image: hive-hudi:4.0.0
  #   container_name: hive-metastore-init
  #   depends_on:
  #     metastore-db:
  #       condition: service_healthy
  #   environment:
  #     HADOOP_CONF_DIR: /opt/hive/conf
  #     HIVE_AUX_JARS_PATH: /opt/hive/auxlib
  #   volumes:
  #     - ./docker/hive/conf/hive-site.xml:/opt/hive/conf/hive-site.xml:ro
  #     - ./docker/hive/conf/core-site.xml:/opt/hive/conf/core-site.xml:ro
  #     - ./docker/hive/auxlib:/opt/hive/auxlib:ro
  #   entrypoint: ["/bin/sh","-lc"]
  #   command:
  #     - |
  #       set -e;
  #       echo "[INIT] Checking metastore schema...";
  #       if /opt/hive/bin/schematool -dbType postgres -info >/dev/null 2>&1; then
  #         echo "[INIT] Schema already initialized -> skip.";
  #         exit 0;
  #       fi

  #       echo "[INIT] Schema not found -> initSchema...";
  #       /opt/hive/bin/schematool -dbType postgres -initSchema --verbose;
  #       echo "[INIT] Done.";
  #   restart: "no"


  # # -------------------- Hive Metastore (Thrift 9083) --------------------
  # hive-metastore:
  #   image: hive-hudi:4.0.0
  #   container_name: hive-metastore
  #   depends_on:
  #     metastore-db:
  #       condition: service_healthy
  #     hive_metastore_init:
  #       condition: service_completed_successfully
  #     minio:
  #       condition: service_started
  #   environment:
  #     HADOOP_CONF_DIR: /opt/hive/conf
  #     HIVE_AUX_JARS_PATH: /opt/hive/auxlib
  #   ports:
  #     - "9083:9083"
  #   volumes:
  #     - ./docker/hive/conf/hive-site.xml:/opt/hive/conf/hive-site.xml:ro
  #     - ./docker/hive/conf/core-site.xml:/opt/hive/conf/core-site.xml:ro
  #     - ./docker/hive/auxlib:/opt/hive/auxlib:ro
  #   entrypoint: ["/bin/sh","-lc"]
  #   command:
  #     - |
  #       set -eux;
  #       exec /opt/hive/bin/hive --service metastore
  #   restart: "no"


  # # -------------------- HiveServer2 (JDBC 10000) --------------------
  # hive-server2:
  #   image: hive-hudi:4.0.0
  #   container_name: hive-server2
  #   depends_on:
  #     hive-metastore:
  #       condition: service_started
  #   environment:
  #     HADOOP_CONF_DIR: /opt/hive/conf
  #     HIVE_AUX_JARS_PATH: /opt/hive/auxlib

  #     # ✅ FIX: HOME phải là path ghi được
  #     HOME: /tmp/hive
  #     HIVE_SERVER2_PID_DIR: /tmp/hive-pids

  #     # ✅ FIX: log dir ghi được
  #     HIVE_LOG_DIR: /tmp/hive-logs
  #     HADOOP_LOG_DIR: /tmp/hadoop-logs

  #     # ✅ JVM options: dùng cách an toàn (không biến thành args của Hive)
  #     HADOOP_CLIENT_OPTS: "-Xms512m -Xmx512m"
  #     # (hoặc bạn có thể dùng JAVA_TOOL_OPTIONS, nhưng HADOOP_CLIENT_OPTS ổn hơn với wrapper scripts)

  #   ports:
  #     - "10000:10000"
  #     - "10002:10002"
  #   volumes:
  #     - ./docker/hive/conf/hive-site.xml:/opt/hive/conf/hive-site.xml:ro
  #     - ./docker/hive/conf/core-site.xml:/opt/hive/conf/core-site.xml:ro
  #     - ./docker/hive/auxlib:/opt/hive/auxlib:ro

  #   entrypoint: ["/bin/sh","-lc"]
  #   command: 
  #     - |
  #       set -eux
  #       mkdir -p /tmp/hive /tmp/hive-pids

  #       rm -f /tmp/hive-pids/hiveserver2.pid /tmp/hiveserver2.pid /tmp/hive/hiveserver2.pid || true

  #       exec /opt/hive/bin/hiveserver2 \
  #       --hiveconf hive.server2.transport.mode=binary \
  #       --hiveconf hive.server2.thrift.bind.host=0.0.0.0 \
  #       --hiveconf hive.server2.thrift.port=10000 \
  #       --hiveconf hive.server2.webui.host=0.0.0.0 \
  #       --hiveconf hive.server2.webui.port=10002


  #   restart: unless-stopped



volumes:
  minio_data:
